{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from finetune.cifar10_models import resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Image Classification with a Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, you were introduced to pre-trained models from a feature-based approach. You saw how we could use techniques like OLS to produce predictions using a naive feature mapping. Then, you implemented custom convolutional filters to produce more substantive and efficient feature representations for the images. Then you saw how a pre-trained model, specifically ResNet18, produced much better featurizations and achieved the highest train/val accuracy for that classification problem. \n",
    "\n",
    "In this part of the notebook, you will walk through a practical exercise of loading a pre-trained model, freezing its hidden layers, and fine-tuning its last fully connected layer for a specific task. This is analagous to doing logistic regression on the pre-trained models featurizations. However, this section will walk you through setting up a basic PyTorch workflow, which consists of model setup, data preparation, training, and evaluation. This is a highly common paradigm in practice, and it is important for you to get experience with this.\n",
    "\n",
    "The setting we will work with first is the problem of classifying between 2 classes of images in CIFAR-10, the dataset you saw in the last notebook. Then we will look at a specific case of where pre-trained models really shine -- when there is a lack of available training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first load a pre-trained ResNet18 model file, if you are interested in how this works, feel free to \n",
    "check out its implementation in ```finetune/cifar10_models```. Also, a minor detail you can feel free to gloss over for now: we need to set the model to evaluation mode (```.eval()```) since we want to prevent any of ResNet's BatchNorm or Dropout layers from being activated when producing the features for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = resnet18(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us take a look at this pre-trained model. What do you notice about its last layer? What kind of layer is it? What are its input and output sizes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Insert Observation Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to fine-tune this model for our task. This means \"freezing\" many of the layers that already have\n",
    "pre-trained weights loaded into them. In order to freeze these layers, we need to tell PyTorch that the parameters of these layers are not meant to get their gradients updated during the training step. Take a look at the PyTorch [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=parameters#torch.nn.Module.parameters) and freeze all of the parameters for ```model_ft```. Hint: what are some attributes of ```nn.Module.parameters```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT CODE HERE\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just frozen all of the model's layers, but we still need to fine-tune the last layer. Since we are working with binary classification, we need to reshape the output of the last fully connected layer. We can do this by creating a new output layer. This does two things -- 1) by default, when creating a new layer, it is \"unfrozen\", meaning its gradients will be updated during training and 2) this allows us to create a layer that is specific to our task. \n",
    "\n",
    "Let us re-assign the model's last fully connected layer to a new linear layer that will be able to handle binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(feature_dim, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to need to source the data for our binary classification task. You worked with CIFAR-10 in the previous notebook. Here we will work with the same dataset, but instead use PyTorch's Data Framework. Let's first import the necessary tools for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important aspect of working with data for ML learning models, is normalization. You've seen in previous assignments the effect of normalization on models. Here, we need to make sure that we normalize our data in the same way that the pre-trained ResNet18 was trained. The following code uses PyTorch's transforms framework to convert image data into Tensors (this rescales images into the range [0,1]) and then normalizes them channel-wise, such that they comply with how ResNet18 was originally trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the neat features of PyTorch is its \"pythonic\" structure. Everything is a class, including the dataset that we want to work with -- CIFAR10. The default ```datasets.CIFAR10``` class gives us a training and validation set with 32x32x3 images from 10 different classes. However, we are interested in binary classification, so we need to modify the CIFAR10 dataset to give us images from only two classes. \n",
    "\n",
    "Take a look at the CIFAR10 class [documentation](https://pytorch.org/docs/stable/_modules/torchvision/datasets/cifar.html#CIFAR10) and think about how you would modify its ```self.targets``` (contains the labels) and ```self.data``` (contains the image tensors) to return data that only comes from classes that we specify in ```include_list```. Hint: use numpy to determine which indices are the indices which can be included; you just need to modify ```self.data``` and ```self.targets```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom PyTorch dataset to create our binary dataset\n",
    "# https://pytorch.org/docs/stable/_modules/torchvision/datasets/cifar.html#CIFAR10\n",
    "\n",
    "\n",
    "class MyCIFAR10(datasets.CIFAR10):\n",
    "    def __init__(self, *args, include_list=[], **kwargs):\n",
    "        super(MyCIFAR10, self).__init__(*args, **kwargs)\n",
    "\n",
    "        if include_list == []: return\n",
    "\n",
    "        ### START CODE HERE\n",
    "        \n",
    "        ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load our CIFAR10 data. The code below calls the CIFAR class you just wrote, with arguments to just load the \"airplane\" and \"automobile\" class (the labels are respectively 0 and 1). Feel free to load whatever two classes you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include = [0,1]\n",
    "train_dataset = MyCIFAR10('data', include_list=include, train=True, download=True, transform=preprocess)\n",
    "val_dataset = MyCIFAR10('data', include_list=include, train=False, download=True, transform=preprocess)\n",
    "label_names = ['airplane', 'automobile']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is our dataset? How many training and validation examples do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT CODE HERE\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to put the Dataset into a format PyTorch models can ingest. The code below loads the training and validation dataset into a DataLoader. This simply shuffles our data and splits the data into batches, so that our gradient doesn't need to be calculated over the entire dataset at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # If your CPU has an issue with some of the later parts, make this 2 or 1\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloaders = {'train' : train_data_loader,\n",
    "              'val' : val_data_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through some of the the inputs and labels in the training dataloader. What are the sizes of each? Why are they shaped this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT CODE HERE\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Insert Observation Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to setup the training function. You saw in the previous notebook how we setup a training function to train a single linear layer. The code below is largely the same, except the model is now much larger. First, let us import some necessary components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a model to be trained, the dataloaders from the previous section, a loss function, a gradient descent optimizer, and a number of epochs. The loss function we'll be using is the same as the loss that is used in Logisitic Regression -- Cross Entropy. And the Optimizer we'll be using is SGD, which you've seen in previous parts of the class.\n",
    "\n",
    "Fill in the part of this function that passes the inputs into the model, calculates the loss, and updates the gradients based on the loss. Hint: look at ```train``` from the previous notebook if you get stuck here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    \n",
    "    model=model.cuda()\n",
    "    \n",
    "    train_loss, val_loss = [], []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    ### START CODE HERE\n",
    "                    \n",
    "                    \n",
    "                    ### END CODE HERE\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss.append(epoch_loss)\n",
    "            else:\n",
    "                val_loss.append(epoch_loss)\n",
    "\n",
    "    return model, {'train' : train_loss, 'val' : val_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train; first, let's make sure that we are only going to update layers in the model that we want to. Does the output of this look right to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_ft.named_parameters():\n",
    "    if param.requires_grad: print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Insert Observation Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to setup the optimizer (SGD) and the loss function (Cross Entropy) along with some other model hyperparamers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.01\n",
    "optimizer = optim.SGD(model_ft.parameters(), lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model. Based on your computer's ability this may take anywhere from 20-30 minutes. During this time, try to understand the ```ConvNet``` code in the next part. If you are running out of time, change the number of epochs to 5 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, ft_log = train_model(model_ft, dataloaders, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just fine-tuned a pre-trained ResNet18 model on two specific classes of CIFAR-10. Now, we need to understand its performance. In order to evaluate it, we need to compare it against something that we are familiar with. Let us create a basic convolutional network and train it using the same dataloader and train function we have from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a very generalized implementation of a model class. As you can see, ```ConvNet``` can be created with a specific num_classes, input_width, input_height, etc. When designing models, make sure to make them generalizable, so that they are flexible if your dataset or task happens to change.\n",
    "\n",
    "Most of this has already been implemented for you, take the time to understand what this is doing, and fill in the ```get_width``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, input_width, input_height, num_channels, num_layers=2, num_filters=[10, 20], kernel_sizes=[5, 5], pool=[True, True]):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        assert len(num_filters) == num_layers, 'length of num_filters must match num_layers'\n",
    "        assert len(kernel_sizes) == num_layers, 'length of kernel_sizes must match num_layers'\n",
    "        assert len(pool) == num_layers, 'length of pool must match num_layers'\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        num_filters = [num_channels] + num_filters\n",
    "\n",
    "        self.widths = [input_width]\n",
    "        self.heights = [input_height]\n",
    "\n",
    "        layers = []\n",
    "        for layer in range(num_layers):\n",
    "            layers.append(nn.Conv2d(num_filters[layer], num_filters[layer + 1], kernel_size=kernel_sizes[layer]))\n",
    "\n",
    "            if pool[layer]:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "            self.widths.append(self.get_width(self.widths[-1], kernel_sizes[layer], pool[layer]))\n",
    "            self.heights.append(self.get_width(self.heights[-1], kernel_sizes[layer], pool[layer]))\n",
    "        \n",
    "        self.convs = torch.nn.Sequential(*layers)\n",
    "\n",
    "        self.ff_in_dim = self.widths[-1] * self.heights[-1] * num_filters[-1]\n",
    "        self.fc1 = nn.Linear(self.ff_in_dim, self.ff_in_dim)\n",
    "        self.fc2 = nn.Linear(self.ff_in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        return self.fc2(x)\n",
    "    \n",
    "    # assume max pool with filter width 2 and stride 2\n",
    "    def get_width(self, input_width, kernel_size, pool):\n",
    "        ### START CODE HERE\n",
    "\n",
    "        \n",
    "        ### END CODE HERE\n",
    "        return conv_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate our ConvNet. Do you notice any similarities or differences with the ResNet from above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = ConvNet(2, 32, 32, 3)\n",
    "conv_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "Insert Observation Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following cells. You will notice that this time, it is much faster to train the model. Why is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.SGD(conv_model.parameters(), lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model, conv_log = train_model(conv_model, dataloaders, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "Insert Observation Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us understand the performance of the pre-trained ResNet versus our basic 2 layer ConvNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(logs):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "    for i, log in enumerate(logs):\n",
    "        ax[i].plot(list(range(0, len(log['train']))), log['train'], label=\"train\")\n",
    "        ax[i].plot(list(range(0, len(log['val']))), log['val'], label=\"val\")\n",
    "        ax[i].legend()\n",
    "\n",
    "    ax[0].set_xlabel(\"Conv Model Loss\")\n",
    "    ax[1].set_xlabel(\"Pre-trained Model Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses([conv_log, ft_log])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the performance of the Conv Model over time? What about the pre-trained model? Is there a difference in val/train loss over time? Why could this be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "Insert Observation Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Practical Use-case of Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed from the last section, that our Conv model started to overfit slightly, while our Pre-trained model did not. This is actually an artifact of the \"generalization\" ability of pre-trained models. Let us go through a practical use-case of when we can make use of this.\n",
    "\n",
    "Suppose that you are a scientist at a company that is tasked with building an image based classification system. And, you are working with the company's expensive, high quality, multi-class, and unsuprisingly proprietary dataset. This is great; however, the data is so expensive that you do not have very many samples to work with. In fact, you only have 20 images per class of these high-quality data points to train on. \n",
    "\n",
    "Now you have a dilemna, should you build a custom model to train with this data, or should you fine-tune a pre-trained model? Let us try both.\n",
    "\n",
    "First, we need to simulate working with a low-sample and multi-class dataset. Let us now use CIFAR-100, a similar dataset to CIFAR10, except with 100 classes. Note that we will still be using a ResNet pre-trained on CIFAR-10 to extract features for these new images; if we still achieve good performance, it will validate the \"transferability\" of the pre-trained model's featurizations.\n",
    "\n",
    "To get the point across, we will be constructing a training dataset that only contains 20 examples per class from CIFAR-100, as opposed to the 600 that it normally has. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # to ensure re-producibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code to select ```sample_size``` number of examples for each of the 100 CIFAR-100 classes. Hint: use ```np.random.choice``` and make use of boolean-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpensiveDataset(datasets.CIFAR100):\n",
    "    def __init__(self, *args, sample_size=20, **kwargs):\n",
    "        super(ExpensiveDataset, self).__init__(*args, **kwargs)\n",
    "\n",
    "        ### START CODE HERE\n",
    "\n",
    "        \n",
    "        ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous section, let us initialize our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = ExpensiveDataset('data', sample_size=20, train=True, download=True, transform=preprocess)\n",
    "val_dataset = ExpensiveDataset('data', sample_size=10, train=False, download=True, transform=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tr/val examples do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT CODE HERE\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "Insert Observation Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the dataloader to handle our new dataset. Hint: refer to above section for an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "### INSERT CODE HERE\n",
    "\n",
    "\n",
    "### END CODE HERE\n",
    "\n",
    "dataloaders = {'train' : train_data_loader,\n",
    "              'val' : val_data_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the dimensions of your data loader inputs and labels are the same shape from before. Output a couple of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT CODE HERE\n",
    "\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instantiate a new pre-trained resnet model following the example from the top of the notebook. What are the changes that you have to make though? Hint: number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT CODE HERE\n",
    "\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following blocks to train the pre-trained model on the new dataset, what do you notice about how long it takes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "lr = 0.01\n",
    "optimizer = optim.SGD(model_ft.parameters(), lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, ft_loss = train_model(model_ft, dataloaders, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "Insert Observation Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instaniate a new ConvNet using the ```ConvNet``` class, similar to the previous section. Hint: what needs to change now that we are in a multi-class setting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT CODE HERE\n",
    "conv_model = \n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following blocks to train your conv model. What do you notice about how long it takes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "lr = 0.01\n",
    "optimizer = optim.SGD(conv_model.parameters(), lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model, conv_loss = train_model(conv_model, dataloaders, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Insert Observation Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following cell to plot the losses on this dataset. What do you notice? How is this different from the previous run? What model should you end up picking to work with in this \"expensive\" data situation? Please be detailed here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses([conv_loss, ft_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Insert Observation Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
