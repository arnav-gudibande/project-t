{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from torchvision import datasets, transforms\n",
    "from finetune.data import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.measure\n",
    "from sklearn.decomposition import PCA\n",
    "from torchvision import models\n",
    "from ipywidgets import interact, widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem overview:\n",
    "In this problem we will discover just how powerful the learned features from pretrained models really are. We will be comparing the performance of 3 different sets of features. The first is the most naive feature set possible: the set of raw pixel values. For the next feature set, you will augment the set raw pixel values with a set of features extracted from hand designed convolutional filters that you will create. And lastly we will compare these two sets of features to the features extracted from a state of the art pretrained model: Resnet18.\n",
    "\n",
    "We will be using the CIFAR10 dataset for all parts of this problem. This is an image classification dataset of 32x32 color images that are in one of ten categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. Learn more about CIFAR10 here https://www.cs.toronto.edu/~kriz/cifar.html. For the purposes of similicity, in this problem we will only classify between airplanes and automobiles, so this is a binary classification problem.\n",
    "\n",
    "We will evaulate the three featureizations described above on three different models. First we will train an ridge penalized OLS model, which you learned about in ee16a. Next we will train a logistic regression model, using scikit's implementation of logistic regression. And lastly we will create a logistic regression model in pytorch and train it via gradient descent.\n",
    "\n",
    "After all of this we will visulize these featurizations by using the PCA dimentionality reducation technique you've learned about in ee16b. Ideally this visualization will give some intuition as to why some featurizations work better than others.\n",
    "\n",
    "And finally we will look at some of the convolutional filters learned by pretrained models, and compare them to the filters you designed by hand. Ideally this exercise will lead you to appraciate just how nuanced and detailed the features learned by these pretrained models really are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the data\n",
    "You don't need to do anything here except run the cell to load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: 0, 1: 1}\n",
    "# train_resnet_features and val_resnet_features contain the pretrained features for each image in the dataset\n",
    "with open('data_ft/train_features.pkl', 'rb') as f:\n",
    "    train_resnet_features = pkl.load(f)\n",
    "    train_resnet_features = [item['features'].squeeze() for item in train_resnet_features if item['y'] in labels]\n",
    "    \n",
    "with open('data_ft/val_features.pkl', 'rb') as f:\n",
    "    val_resnet_features = pkl.load(f)\n",
    "    val_resnet_features = [item['features'].squeeze() for item in val_resnet_features if item['y'] in labels]\n",
    "\n",
    "# train_images and val_images contain the raw images\n",
    "train_dataset, val_dataset, label_names = load_dataset('cifar10')\n",
    "train_images = [x for x, y in train_dataset if y in labels]\n",
    "val_images = [x for x, y in val_dataset if y in labels]\n",
    "\n",
    "# train_y and val_y are arrays of the labels for each item in the images and features arrays\n",
    "y_train = [y for x, y in train_dataset if y in labels]\n",
    "y_val = [y for x, y in val_dataset if y in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize CIFAR 10\n",
    "\n",
    "run the cell blow to vizualize some image samples from CIFAR 10. You should get a feel for what the dataset is like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_val_image(idx):\n",
    "    plt.imshow((val_images[idx].permute(1, 2, 0) * 0.229) + 0.485)\n",
    "    plt.show()\n",
    "\n",
    "interact(show_val_image, idx=widgets.IntSlider(min=0,max=50,step=1,value=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build your own filter\n",
    "\n",
    "Now lets build some convolutional filters !!\n",
    "In lecture this week you learned about convolutional nerual networks. The key component of these networks are the convolutional filters that extract features from images. Of course these filters are typically learned by neural networks. The learned filters extract lots of high level information that is useful not only for the classification task, but they can also be used for other downstream tasks like object detection. This is why pretrained models are so powerful, training a model on image classification learns features that are general enough to be useful elsewhere.\n",
    "\n",
    "To give you a sense for just how detailed and nuanced the learned features are, we wanted to give you a chance to extract your own features by hand designing convolutional filters. You will get to see just how well your features stack up against the features learned by a pretrained resnet.\n",
    "\n",
    "in the cell below create at least 3 3x3 convolutional filters to apply to greyscale versions of these images. These filters should extract features from the images that you think might be useful for classifying between airplanes and automobiles. You are not necessarily restricted to using 3x3 filters, you may also design larger filters, but you are only required to do 3x3.\n",
    "\n",
    "Define your filters as a 3x3 numpy array in the \"filters\" array, we did the first one for you to give you an examples.\n",
    "\n",
    "The cell below will vizualise your filters and the features they extract on a set of images from the training set. To reduce the dimension of the extracted feautres we apply a 3x3 max pool with stride 3 after the convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(image, filter_):\n",
    "    return scipy.signal.convolve2d(image, filter_)\n",
    "\n",
    "def maxpool(image, filter_size):\n",
    "    return skimage.measure.block_reduce(image, filter_size, np.max)\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    return gray\n",
    "\n",
    "filters = [np.array([[-5.0, 0.0, 5.0], [-10.0, 0.0, 10.0], [-5.0, 0.0, 5.0]])]\n",
    "\n",
    "for i in range(10):\n",
    "    norm_image = rgb2gray((train_images[i].permute(1, 2, 0) * 0.229) + 0.485)\n",
    "    print('original image greyscale:')\n",
    "    plt.imshow(norm_image, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    print('format: (filter, after convlution, after max pool)')\n",
    "    for filter_ in filters:\n",
    "        conved = convolve(norm_image, filter_)\n",
    "        max_pooled = maxpool(conved, (3, 3))\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "        ax1.imshow((filter_ - np.min(filter_)) / (np.max(filter_) - np.min(filter_)), cmap='gray')\n",
    "        ax2.imshow(conved, cmap='gray')\n",
    "        ax3.imshow(max_pooled, cmap='gray')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the two cells below to extract your custom features from all the training and validation data, and to preprocess the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_features = []\n",
    "new_val_features = []\n",
    "for i in range(len(train_images)):\n",
    "    norm_image = rgb2gray((train_images[i].permute(1, 2, 0) * 0.229) + 0.485)\n",
    "    new_features = []\n",
    "    for filter_ in filters:\n",
    "        conved = convolve(norm_image, filter_)\n",
    "        max_pooled = maxpool(conved, (3, 3)).reshape(-1)\n",
    "        new_features.extend(max_pooled.tolist())\n",
    "    new_train_features.append(new_features)\n",
    "\n",
    "for i in range(len(val_images)):\n",
    "    norm_image = rgb2gray((val_images[i].permute(1, 2, 0) * 0.229) + 0.485)\n",
    "    new_features = []\n",
    "    for filter_ in filters:\n",
    "        conved = convolve(norm_image, filter_)\n",
    "        max_pooled = maxpool(conved, (3, 3)).reshape(-1)\n",
    "        new_features.extend(max_pooled.tolist())\n",
    "    new_val_features.append(new_features)\n",
    "\n",
    "new_train_features = np.array(new_train_features)\n",
    "new_val_features = np.array(new_val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x_train is the flattened raw pixel features\n",
    "x_train_custom is your custom features concatenated with the flattened raw pixel features\n",
    "x_train_pretrained are the features extracted from a pretrained resnset18 model\n",
    "all arrays use the same y\n",
    "for each of these arrays there is an analogous array for validation data\n",
    "you should use these arrays to train classification models in the next three parts\n",
    "\"\"\"\n",
    "\n",
    "x_train = np.stack([x.reshape(-1).numpy() for x in train_images], axis=0)\n",
    "x_train_custom = np.concatenate((x_train, new_train_features), axis=1)\n",
    "x_val = np.stack([x.reshape(-1).numpy() for x in val_images], axis=0)\n",
    "x_val_custom = np.concatenate((x_val, new_val_features), axis=1)\n",
    "\n",
    "x_train_pretrained = np.stack(train_resnet_features, axis=0)\n",
    "x_val_pretrained = np.stack(val_resnet_features, axis=0)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classifying images with Ridge Regularized OLS\n",
    "Here you will use ridge regression to cllassify between images. We cast binary classification into a regression problem by making the class 0 images y=-1 and the class 1 images y=1. We classify an image as 1 if its output is > 1 and -1 otherwise.\n",
    "\n",
    "Train 3 OLS ridge classifiers. One on the raw image features, one on your handcrafted features + the raw imaage features, and lastly one on the pretrained model features.\n",
    "\n",
    "For each you should report the training and validation accuracy and comment on the results in the text cell below. What ridge setting works best ? Which models perform best ? How do your features seem to stack up against the pretrained features, and how do they compare to the raw pixel features ?\n",
    "\n",
    "Note: don't forget to rescale the y so that it is -1,1 instead of 0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ols_ridge(x_train, y_train, x_val, y_val, lambda_):\n",
    "    # lambda_ is the regularization constant\n",
    "    \"\"\"\n",
    "    your code here\n",
    "    \"\"\"\n",
    "    return train_acc, val_acc\n",
    "\n",
    "y_train_ols = 2 * y_train - 1\n",
    "y_val_ols = 2 * y_val - 1\n",
    "\n",
    "#set this constant\n",
    "lambda_ = None\n",
    "\n",
    "train_acc, val_acc = ols_ridge(x_train, y_train_ols, x_val, y_val_ols, lambda_=lambda_)\n",
    "print('raw image:')\n",
    "print('ols training accuracy: {acc}'.format(acc=train_acc))\n",
    "print('ols validation accuracy: {acc}'.format(acc=val_acc))\n",
    "\n",
    "train_acc, val_acc = ols_ridge(x_train_custom, y_train_ols, x_val_custom, y_val_ols, lambda_=lambda_)\n",
    "print('raw image + custom features:')\n",
    "print('ols training accuracy: {acc}'.format(acc=train_acc))\n",
    "print('ols validation accuracy: {acc}'.format(acc=val_acc))\n",
    "\n",
    "train_acc, val_acc = ols_ridge(x_train_pretrained, y_train_ols, x_val_pretrained, y_val_ols, lambda_=lambda_)\n",
    "print('pretrained features:')\n",
    "print('ols training accuracy: {acc}'.format(acc=train_acc))\n",
    "print('ols validation accuracy: {acc}'.format(acc=val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "### comment on your observations here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classifying images with logistic regression\n",
    "Obviously this is a classification problem, not a regression problem, so really it is a better idea to use logistic regression than OLS for this kind of task.\n",
    "\n",
    "Train 3 logistic regression classifiers using the scikit LogisticRegression class. Report the same set of accuracies as the previous part. And comment on the results in the text cell below. What L2 penality worked best ? How did your custom features stack up against the pretrained model, and how did they compare to the raw pixel features ? Did logistic regression perform better or worse than OLS ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg(x_train, y_train, x_val, y_val, lamb):\n",
    "    # lamb is the regularization constant\n",
    "    \"\"\"\n",
    "    your code here\n",
    "    \"\"\"\n",
    "    return train_acc, val_acc\n",
    "\n",
    "# set this constant\n",
    "lamb = None\n",
    "\n",
    "train_acc, val_acc = log_reg(x_train, y_train, x_val, y_val, lamb=lamb)\n",
    "print('raw image:')\n",
    "print('logistic regression training accuracy: {acc}'.format(acc=train_acc))\n",
    "print('logistic regression validation accuracy: {acc}'.format(acc=val_acc))\n",
    "\n",
    "train_acc, val_acc = log_reg(x_train_custom, y_train, x_val_custom, y_val, lamb=lamb)\n",
    "print('raw image + custom features:')\n",
    "print('logistic regression training accuracy: {acc}'.format(acc=train_acc))\n",
    "print('logistic regression validation accuracy: {acc}'.format(acc=val_acc))\n",
    "\n",
    "train_acc, val_acc = log_reg(x_train_pretrained, y_train, x_val_pretrained, y_val, lamb=lamb)\n",
    "print('pretrained features:')\n",
    "print('logistic regression training accuracy: {acc}'.format(acc=train_acc))\n",
    "print('logistic regression validation accuracy: {acc}'.format(acc=val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "### comment on your observations here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a second logistic regression implementation\n",
    "\n",
    "In the previous part you used the SciKit implmentation of logistic regression to classify images. Here do the exact same thing as the previous part, except train it with stochastic gradient descent in pytorch. Report the same accuracy numbers. Your results should be similar to the previous part. You can use the weight_decay parameter in torch.optim.SGD to control L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, x_val, y_val, lamb, steps, lr):\n",
    "    # lamb is the regularization constant\n",
    "    \"\"\"\n",
    "    your code here\n",
    "    \"\"\"\n",
    "\n",
    "#set these constants\n",
    "steps = None\n",
    "lr = None\n",
    "lamb = None\n",
    "\n",
    "print('raw image:')\n",
    "train(torch.tensor(x_train), torch.tensor(y_train), torch.tensor(x_val), torch.tensor(y_val), lamb=lamb, steps=steps, lr=lr)\n",
    "print('raw image + custom features:')\n",
    "train(torch.tensor(x_train_custom).float(), torch.tensor(y_train), torch.tensor(x_val_custom).float(), torch.tensor(y_val), lamb=lamb, steps=steps, lr=lr)\n",
    "print('pretrained features:')\n",
    "train(torch.tensor(x_train_pretrained).float(), torch.tensor(y_train), torch.tensor(x_val_pretrained).float(), torch.tensor(y_val), lamb=lamb, steps=steps, lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature visualization\n",
    "\n",
    "In the following cells, run PCA on the raw pixel features from the training data, and project the validation features onto the two principal components with the largest singular values. Use these projections to visualize the data in two dimensions. Also plot the magnitude of top 200 singular values.\n",
    "\n",
    "Repeat this for the other two sets of features in the next two cells, and then comment on your observations in the the text cell below. What differences do you see between the visualizations of the three featurizations ? Do these visualizations help explain any of the accuracy differences you saw in the previous parts ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "your code here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "your code here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "your code here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "### comment on your observations here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do pretrained models do it ?\n",
    "\n",
    "Clearly these pretrained features are on another level. We'd be extreamly impressed if your handcrafted features  even came close to the pretrained features in accuracy. So why are these pretrained features so good ?\n",
    "\n",
    "Typically pretrained models are trained on the ImageNet dataset. A massive and diverse dataset of color images from 1000 different classes. When these models are trained to classify on this massive dataset, they learn a set of very powerful image features.\n",
    "\n",
    "This is the website for the image net dataset http://www.image-net.org/index. You can search for images by class on website. Explore the dataset and get a feel for the variety of images that these models are trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualizing pretrained model filter\n",
    "So you tried to come up with handcrafted filters, but they probably didn't outperform the pretrained models. So what filters do these pretrained models learn ? What is so much better about them ? Run the cell below to visualize the filters learned by a model pretrained on imagenet. These filters are just from the first layer of the network. After the first layer, we can't visualize the filters as easily because they take in more than 3 channels of input. But you can proabably guess that these deeper filters learn to extract even more complex features.\n",
    "\n",
    "Report your observations in the text cell below. What kinds of features do these filters seem to be extracting ? How do they compare to your handcrafted filters ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_net_model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(8, 8, figsize=(8, 8))\n",
    "for i in range(image_net_model.conv1.weight.shape[0]):\n",
    "    filter_ = image_net_model.conv1.weight[i].permute(1, 2, 0).clone().detach().numpy()\n",
    "    filter_ = (filter_ - np.min(filter_)) / (np.max(filter_) - np.min(filter_))\n",
    "    axs[i // 8, i % 8].imshow(filter_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "### comment on your observations here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optional challenge for bonus points\n",
    "\n",
    "If you can get your hand crafted features to exceed 90% validation accuracy on your SGD implementation of logistic regression, you will recieve 1 bonus point. If you exceed 92.5% you will recieve 2 bonus points. And if you exceed 95% you will recieve 3 bonus points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
